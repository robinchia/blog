---
layout: post
title: "多IDC的数据分布设计(一)"
categories: 架构师
tags: 
 - 架构师
 - 架构
--- 

# 多IDC的数据分布设计(一)

# [Tim[后端技术]](http://timyang.net/ "Home")

Tim's blog, 关于后端架构、互联网技术、分布式、大型网络应用、NoSQL、技术感悟等

[Home](http://timyang.net/) | [About](http://timyang.net/about/) | [English version](http://timyang.net/tag/English/) | [留言(Guestbook)](http://timyang.net/about/#comment) | [![]()](http://timyang.net/feed/)[订阅RSS](http://timyang.net/feed/) [![Add to Google]()](http://fusion.google.com/add?source=atgs&feedurl=http%3A//timyang.net/feed/)
1. 
* Email: ![]()

* 
### Most Commented

* [MemcacheDB, Tokyo Tyrant, Redis performance test](http://timyang.net/data/mcdb-tt-redis/ "MemcacheDB, Tokyo Tyrant, Redis performance test") (101)
* [C, Erlang, Java and Go Web Server performance test](http://timyang.net/programming/c-erlang-java-performance/ "C, Erlang, Java and Go Web Server performance test") (85)
* [About Tim Yang](http://timyang.net/about/ "About Tim Yang") (83)
* [Redis几个认识误区](http://timyang.net/data/redis-misunderstanding/ "Redis几个认识误区") (62)
* [Memcache mutex设计模式](http://timyang.net/programming/memcache-mutex/ "Memcache mutex设计模式") (47)
* [构建可扩展的微博架构(qcon beijing 2010演讲)](http://timyang.net/architecture/microblog-design-qcon-beijing/ "构建可扩展的微博架构(qcon beijing 2010演讲)") (39)
* [某分布式应用实践一致性哈希的一些问题](http://timyang.net/architecture/consistent-hashing-practice/ "某分布式应用实践一致性哈希的一些问题") (38)
* [为什么优秀开发者进入Google后就不参与开源了](http://timyang.net/google/open-source/ "为什么优秀开发者进入Google后就不参与开源了") (33)
* [MacBook Air与工作效率](http://timyang.net/misc/macbook-air-productive/ "MacBook Air与工作效率") (32)
* [Memcached数据被踢(evictions>0)现象分析](http://timyang.net/data/memcached-lru-evictions/ "Memcached数据被踢(evictions>0)现象分析") (30)

* 
### Recent Posts

* [演讲小组的第一次活动](http://timyang.net/tao/speech-practice/ "演讲小组的第一次活动")
* [“connect the dots” 随想](http://timyang.net/tao/connect-the-dots/ "“connect the dots” 随想")
* [跨领域人才](http://timyang.net/tao/talents/ "跨领域人才")
* [产品下线杂谈](http://timyang.net/product/discontinued-servic/ "产品下线杂谈")
* [当我谈演讲时候，我谈些什么](http://timyang.net/misc/speech/ "当我谈演讲时候，我谈些什么")
* [案例与故障的知识库](http://timyang.net/architecture/k/ "案例与故障的知识库")
* [团队中的为师之道](http://timyang.net/management/learning-tao/ "团队中的为师之道")
* [有感Google的混合研究方法](http://timyang.net/google/hybrid-research/ "有感Google的混合研究方法")
* [谈技术人员“转正”](http://timyang.net/management/probation/ "谈技术人员“转正”")
* [微信架构的启示](http://timyang.net/architecture/notes-weixin/ "微信架构的启示")
* [MacBook Air与工作效率](http://timyang.net/misc/macbook-air-productive/ "MacBook Air与工作效率")
* [Notes about Timelines @ Twitter](http://timyang.net/architecture/notes-timelines-twitter/ "Notes about Timelines @ Twitter")
* [技术工程师的能力与目标](http://timyang.net/management/engineer-performance/ "技术工程师的能力与目标")
* [技术方案评审](http://timyang.net/management/design-review/ "技术方案评审")
* [谈技术团队目标](http://timyang.net/management/planning/ "谈技术团队目标")

* 
### Recent Comments

* [Redis几个认识误区 | 创造](http://emake.info/the-redis-several-misunderstandings.html) on [Redis几个认识误区](http://timyang.net/data/redis-misunderstanding/comment-page-2/#comment-224409)
* [Redis几个认识误区 | 创造](http://emake.info/the-redis-several-misunderstandings.html) on [MemcacheDB, Tokyo Tyrant, Redis performance test](http://timyang.net/data/mcdb-tt-redis/comment-page-3/#comment-224408)
* [甄码农](http://outofmemory.cn/) on [团队中的为师之道](http://timyang.net/management/learning-tao/comment-page-1/#comment-224161)
* [甄码农](http://outofmemory.cn/) on [跨领域人才](http://timyang.net/tao/talents/comment-page-1/#comment-224158)
* 阿秀 on [About Tim Yang](http://timyang.net/about/comment-page-2/#comment-223584)

* 
### Categories

* [data](http://timyang.net/category/data/ "View all posts filed under data")
* [Erlang](http://timyang.net/category/erlang/ "Erlang语言")
* [Google](http://timyang.net/category/google/ "View all posts filed under Google")
* [Java](http://timyang.net/category/java/ "View all posts filed under Java")
* [Linux](http://timyang.net/category/linux/ "View all posts filed under Linux")
* [Lua](http://timyang.net/category/lua/ "View all posts filed under Lua")
* [Python](http://timyang.net/category/python/ "View all posts filed under Python")
* [SNS](http://timyang.net/category/sns/ "View all posts filed under SNS")
* [tech](http://timyang.net/category/tech/ "View all posts filed under tech")
* [Web](http://timyang.net/category/web/ "View all posts filed under Web")
* [产品](http://timyang.net/category/product/ "View all posts filed under 产品")
* [分布式](http://timyang.net/category/distributed/ "分布式算法及思想")
* [技术管理](http://timyang.net/category/management/ "View all posts filed under 技术管理")
* [架构](http://timyang.net/category/architecture/ "后端架构")
* [编程](http://timyang.net/category/programming/ "View all posts filed under 编程")
* [随想](http://timyang.net/category/tao/ "View all posts filed under 随想")
* [非技术](http://timyang.net/category/misc/ "非技术话题")

* 
### Archives

* [October 2012](http://timyang.net/2012/10/ "October 2012") (5)
* [September 2012](http://timyang.net/2012/09/ "September 2012") (4)
* [May 2012](http://timyang.net/2012/05/ "May 2012") (3)
* [February 2012](http://timyang.net/2012/02/ "February 2012") (3)
* [January 2012](http://timyang.net/2012/01/ "January 2012") (2)
* [December 2011](http://timyang.net/2011/12/ "December 2011") (1)
* [August 2011](http://timyang.net/2011/08/ "August 2011") (1)
* [July 2011](http://timyang.net/2011/07/ "July 2011") (1)
* [April 2011](http://timyang.net/2011/04/ "April 2011") (2)
* [February 2011](http://timyang.net/2011/02/ "February 2011") (1)
* [January 2011](http://timyang.net/2011/01/ "January 2011") (2)
* [December 2010](http://timyang.net/2010/12/ "December 2010") (2)
* [November 2010](http://timyang.net/2010/11/ "November 2010") (2)
* [September 2010](http://timyang.net/2010/09/ "September 2010") (1)
* [August 2010](http://timyang.net/2010/08/ "August 2010") (1)
* [July 2010](http://timyang.net/2010/07/ "July 2010") (3)
* [June 2010](http://timyang.net/2010/06/ "June 2010") (2)
* [May 2010](http://timyang.net/2010/05/ "May 2010") (2)
* [April 2010](http://timyang.net/2010/04/ "April 2010") (1)
* [March 2010](http://timyang.net/2010/03/ "March 2010") (4)
* [February 2010](http://timyang.net/2010/02/ "February 2010") (1)
* [January 2010](http://timyang.net/2010/01/ "January 2010") (1)
* [December 2009](http://timyang.net/2009/12/ "December 2009") (3)
* [November 2009](http://timyang.net/2009/11/ "November 2009") (3)
* [October 2009](http://timyang.net/2009/10/ "October 2009") (3)
* [September 2009](http://timyang.net/2009/09/ "September 2009") (4)
* [August 2009](http://timyang.net/2009/08/ "August 2009") (5)
* [July 2009](http://timyang.net/2009/07/ "July 2009") (6)
* [June 2009](http://timyang.net/2009/06/ "June 2009") (5)
* [May 2009](http://timyang.net/2009/05/ "May 2009") (11)
* [April 2009](http://timyang.net/2009/04/ "April 2009") (7)
* [March 2009](http://timyang.net/2009/03/ "March 2009") (3)
* [February 2009](http://timyang.net/2009/02/ "February 2009") (2)
* [January 2009](http://timyang.net/2009/01/ "January 2009") (2)
* [December 2008](http://timyang.net/2008/12/ "December 2008") (2)
* 
### Feeds

* [Entries (RSS)](http://timyang.net/feed/)
* [Comments (RSS)](http://timyang.net/comments/feed/)
*

## Archive for the ‘分布式’ Category

## [多IDC的数据分布设计(一)](http://timyang.net/distributed/multi-idc-consensus/ "Permanent Link to 多IDC的数据分布设计(一)")

Tuesday, Feb 2nd, 2010 by Tim | **[13 Comments](http://timyang.net/distributed/multi-idc-consensus/#comments)**
Filed under: [分布式](http://timyang.net/category/distributed/ "View all posts in 分布式") | Tags: [2PC](http://timyang.net/tag/2pc/), [3PC](http://timyang.net/tag/3pc/), [consensus](http://timyang.net/tag/consensus/), [paxos](http://timyang.net/tag/paxos/), [Three-phase commit](http://timyang.net/tag/three-phase-commit/), [Two-phase commit](http://timyang.net/tag/two-phase-commit/)

上个月跟某个朋友谈及多IDC数据同时读写访问的问题([tweet](http://twitter.com/xmpp/status/7625866165))，当时觉得有不少解决方案，但觉得思路还不够清晰。最近看了Google App Engine工程师Ryan Barrett介绍GAE后端数据服务的演讲稿[Transactions Across Datacenters](http://snarfed.org/space/transactions_across_datacenters_io.html)([视频](http://www.youtube.com/watch?v=srOgpXECblk))，用Ryan的方法来分析这个问题后就豁然开朗。

按Ryan的方法，多IDC实现有以下几种思路。

## 一、Master/slave

这个是多机房数据访问最常用的方案，一般的需求用此方案即可。因此大家也经常提到“premature optimization is the root of all evil”。
**优点：**利用mysql replication即可实现，成熟稳定。
**缺点：**写操作存在单点故障，master坏掉之后slave不能写。另外slave的延迟也是个困扰人的小问题。

## 二、Multi-master

[Multi-master](http://en.wikipedia.org/wiki/Multi-master_replication)指一个系统存在多个master, 每个master都具有read-write能力，需根据时间戳或业务逻辑合并版本。比如分布式版本管理系统git可以理解成multi-master模式。具备最终一致性。多版本数据修改可以借鉴Dynamo的vector clock等方法。

**优点：**解决了单点故障。
**缺点：**不易实现一致性，合并版本的逻辑复杂。

## 三、Two-phase commit(2PC)

[Two-phase commit](http://en.wikipedia.org/wiki/Two-phase_commit_protocol)是一个比较简单的一致性算法。由于一致性算法通常用神话(如Paxos的The Part-Time Parliament论文)来比喻容易理解，下面也举个类似神话的例子。

某班要组织一个同学聚会，前提条件是所有参与者同意则活动举行，任意一人拒绝则活动取消。用2PC算法来执行过程如下

### Phase 1

**Prepare:**组织者(coordinator)打电话给所有参与者(participant) ，同时告知参与者列表。
**Proposal:** 提出周六2pm-5pm举办活动。
**Vote:** participant需vote结果给coordinator：accept or reject。
**Block:** 如果accept, participant锁住周六2pm-5pm的时间，不再接受其他请求。

### Phase 2

**Commit:** 如果所有参与者都同意，组织者coodinator通知所有参与者commit, 否则通知abort，participant解除锁定。

### Failure 典型失败情况分析

**Participant failure:**
任一参与者无响应，coordinator直接执行abort
**Coordinator failure:**
**Takeover:** 如果participant一段时间没收到cooridnator确认(commit/abort)，则认为coordinator不在了。这时候可自动成为Coordinator备份(watchdog)
**Query:** watchdog根据phase 1接收的participant列表发起query
**Vote:** 所有participant回复vote结果给watchdog, accept or reject
**Commit:** 如果所有都同意，则commit, 否则abort。

**优点：**实现简单。
**缺点：**所有参与者需要阻塞(block)，throughput低；无容错机制，一节点失败则整个事务失败。

## 四、Three-phase commit (3PC)

[Three-phase commit](http://en.wikipedia.org/wiki/Three-phase_commit_protocol)是一个2PC的改进版。2PC有一些很明显的缺点，比如在coordinator做出commit决策并开始发送commit之后，某个participant突然crash，这时候没法abort transaction, 这时候集群内实际上就存在不一致的情况，crash恢复后的节点跟其他节点数据是不同的。因此3PC将2PC的commit的过程1分为2,分成preCommit及commit, 如图。
![]( "3PC")
(图片来源：[http://en.wikipedia.org/wiki/File:Three-phase_commit_diagram.png](http://en.wikipedia.org/wiki/File:Three-phase_commit_diagram.png))

从图来看，cohorts(participant)收到preCommit之后，如果没收到commit, 默认也执行commit, 即图上的timeout cause commit。

如果coodinator发送了一半preCommit crash, watchdog接管之后通过query, 如果有任一节点收到commit, 或者全部节点收到preCommit, 则可继续commit, 否则abort。

**优点：**允许发生单点故障后继续达成一致。
**缺点：**网络分离问题，比如preCommit消息发送后突然两个机房断开，这时候coodinator所在机房会abort, 另外剩余replicas机房会commit。

## 五、Paxos

Google Chubby的作者Mike Burrows说过， “there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos. 意即“世上只有一种一致性算法，那就是Paxos”，所有其他一致性算法都是Paxos算法的不完整版。相比2PC/3PC, Paxos算法的改进

* P1a. 每次Paxos实例执行都分配一个编号，编号需要递增，每个replica不接受比当前最大编号小的提案
* P2. 一旦一个 value v 被replica通过，那么之后任何再批准的 value 必须是 v，即没有拜占庭将军(Byzantine)问题。拿上面请客的比喻来说，就是一个参与者一旦accept周六2pm-5pm的proposal, 就不能改变主意。以后不管谁来问都是accept这个value。
* 一个proposal只需要多数派同意即可通过。因此比2PC/3PC更灵活，在一个2f+1个节点的集群中，允许有f个节点不可用。

另外Paxos还有很多约束的细节，特别是Google的chubby从工程实现的角度将Paxos的细节补充得非常完整。比如如何避免Byzantine问题，由于节点的持久存储可能会发生故障，Byzantine问题会导致Paxos算法P2约束失效。

以上几种方式原理比较如下

[![]( "idc-transaction")]()

(图片来源：[http://snarfed.org/space/transactions_across_datacenters_io.html](http://snarfed.org/space/transactions_across_datacenters_io.html))

后文会继续比较实践环境选取何种策略合适。

（PS: 写完后在Google Reader上发现本文跟王建硕最近发表的《[关于两个机房的讨论](http://home.wangjianshuo.com/cn/20100201_aeaeaecee.htm)》文章有点类似，特别是本文一、二方式。不过他的文章偏MySQL的实现，我的重点是一致性算法，大家可以有选择性的阅读。）

## [Paxos在大型系统中常见的应用场景](http://timyang.net/distributed/paxos-scenarios/ "Permanent Link to Paxos在大型系统中常见的应用场景")

Wednesday, Sep 23rd, 2009 by Tim | **[5 Comments](http://timyang.net/distributed/paxos-scenarios/#comments)**
Filed under: [分布式](http://timyang.net/category/distributed/ "View all posts in 分布式") | Tags: [chubby](http://timyang.net/tag/chubby/), [paxos](http://timyang.net/tag/paxos/), [zookeeper](http://timyang.net/tag/zookeeper/)

在分布式算法领域，有个非常重要的算法叫Paxos, 它的重要性有多高呢，Google的Chubby [1]中提到
all working protocols for asynchronous consensus we have so far encountered have Paxos at their core.

关于Paxos算法的详述在维基百科中有更多介绍，中文版介绍的是choose value的规则[2]，英文版介绍的是Paxos 3 phase commit的流程[3]，中文版不是从英文版翻译而是独立写的，所以非常具有互补性。Paxos算法是由Leslie Lamport提出的，他在Paxos Made Simple[4]中写道

The Paxos algorithm, when presented in plain English, is very simple.

当你研究了很长一段时间Paxos算法还是有点迷糊的时候，看到上面这句话可能会有点沮丧。但是公认的它的算法还是比较繁琐的，尤其是要用程序员严谨的思维将所有细节理清的时候，你的脑袋里更是会充满了问号。Leslie Lamport也是用了长达9年的时间来完善这个算法的理论。

实际上对于一般的开发人员，我们并不需要了解Paxos所有细节及如何实现，只需要知道Paxos是一个分布式选举算法就够了。本文主要介绍一下Paxos常用的应用场合，或许有一天当你的系统增大到一定规模，你知道有这样一个技术，可以帮助你正确及优雅的解决技术架构上一些难题。

1. database replication, log replication等， 如bdb的数据复制就是使用paxos兼容的算法。Paxos最大的用途就是保持多个节点数据的一致性。

2. naming service, 如大型系统内部通常存在多个接口服务相互调用。
1) 通常的实现是将服务的ip/hostname写死在配置中，当service发生故障时候，通过手工更改配置文件或者修改DNS指向的方法来解决。缺点是可维护性差，内部的单元越多，故障率越大。
2) LVS双机冗余的方式，缺点是所有单元需要双倍的资源投入。
通过Paxos算法来管理所有的naming服务，则可保证high available分配可用的service给client。象ZooKeeper还提供watch功能，即watch的对象发生了改变会自动发notification, 这样所有的client就可以使用一致的，高可用的接口。

3.config配置管理
1) 通常手工修改配置文件的方法，这样容易出错，也需要人工干预才能生效，所以节点的状态无法同时达到一致。
2) 大规模的应用都会实现自己的配置服务，比如用http web服务来实现配置中心化。它的缺点是更新后所有client无法立即得知，各节点加载的顺序无法保证，造成系统中的配置不是同一状态。

4.membership用户角色/access control list, 比如在权限设置中，用户一旦设置某项权限比如由管理员变成普通身份，这时应在所有的服务器上所有远程CDN立即生效，否则就会导致不能接受的后果。

5. 号码分配。通常简单的解决方法是用数据库自增ID, 这导致数据库切分困难，或程序生成GUID, 这通常导致ID过长。更优雅的做法是利用paxos算法在多台replicas之间选择一个作为master, 通过master来分配号码。当master发生故障时，再用paxos选择另外一个master。

这里列举了一些常见的Paxos应用场合，对于类似上述的场合，如果用其它解决方案，一方面不能提供自动的高可用性方案，同时也远远没有Paxos实现简单及优雅。

Yahoo!开源的ZooKeeper [5]是一个开源的类Paxos实现。它的编程接口看起来很像一个可提供强一致性保证的分布式小文件系统。对上面所有的场合都可以适用。但可惜的是，ZooKeeper并不是遵循Paxos协议，而是基于自身设计并优化的一个2 phase commit的协议，因此它的理论[6]并未经过完全证明。但由于ZooKeeper在Yahoo!内部已经成功应用在HBase, Yahoo! Message Broker, Fetch Service of Yahoo! crawler等系统上，因此完全可以放心采用。

另外选择Paxos made live [7]中一段实现体会作为结尾。
*  There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system. In order to build a real-world system, an expert needs to use numerous ideas scattered in the literature and make several relatively small protocol extensions. The cumulative effort will be substantial and the final system will be based on an unproven protocol.
* 由于chubby填补了Paxos论文中未提及的一些细节，所以最终的实现系统不是一个理论上完全经过验证的系统

* The fault-tolerance computing community has not developed the tools to make it easy to implement their algorithms.
* 分布式容错算法领域缺乏帮助算法实现的的配套工具, 比如编译领域尽管复杂，但是yacc, ANTLR等工具已经将这个领域的难度降到最低。

* The fault-tolerance computing community has not paid enough attention to testing, a key ingredient for building fault-tolerant systems.
* 分布式容错算法领域缺乏测试手段

这里要补充一个背景，就是要证明分布式容错算法的正确性通常比实现算法还困难，Google没法证明Chubby是可靠的，Yahoo!也不敢保证它的ZooKeeper理论正确性。大部分系统都是靠在实践中运行很长一段时间才能谨慎的表示，目前系统已经基本没有发现大的问题了。

**Resources**
[1] [The Chubby lock service for loosely-coupled distributed systems](http://labs.google.com/papers/chubby-osdi06.pdf) (PDF)
[2] [http://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95](http://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95)
[3] [http://en.wikipedia.org/wiki/Paxos_algorithm](http://en.wikipedia.org/wiki/Paxos_algorithm)
[4] [Paxos Made Simple](http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf) (PDF)
[5] [ZooKeeper](http://hadoop.apache.org/zookeeper/)
[6] [The life and times of a zookeeper](http://portal.acm.org/citation.cfm?id=1583991.1584007)
[7] [Paxos Made Live – An Engineering Perspective](http://labs.google.com/papers/paxos_made_live.pdf) (PDF)

Except where otherwise noted, content on this site is licensed under a [Creative Commons Attribution 3.0](http://creativecommons.org/licenses/by/3.0/) License
